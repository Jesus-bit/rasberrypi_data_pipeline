---

## ‚úÖ Pipeline Stages

### 1Ô∏è‚É£ Extract Logs
- Connect to S3 bucket.
- Download all `.log` files from the `raw-logs` folder.

### 2Ô∏è‚É£ Transform Logs
- Filter only lines with `ERROR` or `500` status codes.
- Store the filtered logs in-memory.

### 3Ô∏è‚É£ Load Logs
- Upload the filtered logs to the `processed-logs` folder in the same bucket.

---

## üõ†Ô∏è Technology Stack
- **Airflow DAGs** for pipeline coordination.
- **Python (Boto3)** for S3 interaction.
- **Docker** for Airflow environment.
- **GitHub Actions / GitLab CI/CD** for deployment.

---

## üõ†Ô∏è Installation Guide

### 1Ô∏è‚É£ Install Docker and Docker Compose
Ensure Docker is installed on your machine. If not, install it from [Docker Official Site](https://www.docker.com/).

### 2Ô∏è‚É£ Clone the Repository
```
git clone https://github.com/your-repo/my_s3_airflow_pipeline.git
cd my_s3_airflow_pipeline
```

### 3Ô∏è‚É£ Set up AWS Credentials
Configure your AWS credentials in `cloud/aws/s3_config.json`.

### 4Ô∏è‚É£ Start Airflow with Docker Compose
```
cd airflow
docker-compose up -d
```

### 5Ô∏è‚É£ Access Airflow UI
Visit `http://localhost:8080` and activate the DAG `s3_etl_pipeline`.

---

## üöÄ Deployment on Cloud

### 1Ô∏è‚É£ Deploy to AWS Managed Airflow
- Push the DAG to S3 bucket assigned to AWS Managed Airflow.

### 2Ô∏è‚É£ Deploy to Cloud Composer (GCP)
- Upload the DAG to the DAG bucket in Cloud Composer.

---

## üìà Airflow DAG Code
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import boto3
import pandas as pd
import io

# AWS S3 Configuration
BUCKET_NAME = 'my-bucket'
RAW_LOGS_FOLDER = 'raw-logs/'
PROCESSED_LOGS_FOLDER = 'processed-logs/'